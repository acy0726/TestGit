https://github.com/pbiecek/xai_resources

Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations


심층 신경망은 복잡하고 불투명합니다.
다양한 중요하고 안전에 중요한 영역에서 응용 프로그램을 시작할 때 사용자는 출력 예측을 설명하는 방법을 찾습니다. 우리는 CNN에 포함 된 두드러진 개념에 대한 인과 모델을 구성하여 심층 신경망을 설명하는 접근법을 개발합니다. 인간이 이해할 수없는 네트워크 활성화 표현을 추출하도록 훈련 된 자동 인코더를 사용하여 대상 네트워크 전체에서 두드러진 개념을 추출하는 방법을 개발합니다. 그런 다음 이미지 분류를 설명하기 위해 추출 된 개념을 변수로 사용하여 베이지안 인과 모델을 작성합니다. 마지막으로,이 인과 관계 모델을 사용하여 최종 분류에 중대한 인과 관계가있는 특징을 식별하고 시각화합니다.

인공 지능 (AI) 가능 시스템은 우리 사회에서 점점 더 중요한 역할을 수행하고 있으며 상업 및 정부 기관의 운영 방식을 크게 방해하고 있습니다. 특히 DNN (Deep Neural Networks)을 사용한 머신 러닝 (ML)의 발전 속도는 엄청납니다. AI 지원 시스템이 Go [Silver et al., 2016], 자율 주행 [Bojarski 및 기타, 2016], 인구 조사 데이터 예측 [Gebru et al., 2017]. 그러나 이러한 놀라운 발전은 비용이 많이 듭니다. DNN은 복잡하고 불투명하며 대용량 데이터 세트에 대해 훈련해야하는 수백만 개의 매개 변수에서 그 힘을 이끌어냅니다. 따라서 DNN이 특정 결론에 도달 한 이유를 이해하거나 설명하는 것은 어려운 작업입니다. 그러나 AI 지원 시스템이 우리 삶에서 더 널리 보급됨에 따라 이러한 설명 가능성과 해석 불가능 성이 심각한 사회적 결과를 초래합니다. 예를 들어, ML은 현재 형사 사법 시스템 [Berk, 2017], 아동 복지 시스템의 개입 [Cuccaro-Alamin et al., 2017], 방사선 영상의 암 [Esteva et al., 2017]. 이러한 응용 프로그램에서 설명이 부족하면 생명 또는 사망에 영향을 줄 수 있습니다.

AI와 사회 과학 공동체에서는 설명과 해석의 필요성이 눈에 띄지 않았습니다. 고유 한 설명 가능성을 가진 DNN을 부여하거나 더 설명 가능한 방법을 사용하여 DNN의 예측력을 달성하기위한 중요한 발전이 이루어지고 있습니다. 특히, 이미지 영역에서 CNN (Convolutional Neural Networks)을 설명하는 데 상당한 진전이 있었으며, 설명을 쉽게 표현할 수 있었기 때문입니다. 예를 들어, GradCAM [Selvaraju et al., 2016] 및 LRP [Binder et al., 2016]는 CNN의 출력에 대한 이미지의 픽셀 관련성을 나타내는 돌출 맵을 생성하는 두 가지 인기있는 방법입니다.

이러한 방법은 설명 가능성을 크게 향상 시켰지만, 인간이 진정으로 이해하는 데 필요한 한 가지 중요한 요소가 부족합니다. 인과 해석이 제한되어 있습니다. 설명 수단으로서의 인과 관계는 AI 공동체 내에 뿌리를두고있다 [Pearl, 2009]. DNN 운영에 대한 인과 적 설명은 DNN 출력 (즉, 사람)의 최종 소비자에게 DNN 또는 그 입력에 대해 어떻게 그리고 무엇이 변경 될 수 있는지에 대한 이해를 제공하여 출력에 영향을 미친다. 신용 점수 나 자금 세탁과 같은 매우 민감한 영역에서는 시스템 구현자가 정부 모델에 ML 모델의 운영을 정당화해야하기 때문에 인과 적 설명이 중요합니다. 그러나 한 가지 주요 과제는 인과 적 설명이 사람이 이해할 수있는 개념이나 변수의 관점에서 공식화되어야한다는 것입니다. 그렇지 않으면 설명이 원래 모델처럼 난독 화 될 수 있습니다.

본 백서에서 살펴본 가설은 임의의 인과 적 개입과 쿼리를 허용하는 DNN 운영에 대한 인간이 이해할 수있는 인과 적 모델이 설명 및 해석 성을위한 효과적이고 종종 필요한 도구라는 점을 제시합니다. 임의의 개입을 통해 사용자는 DNN 입력에서 도메인의 저수준 기능, 인간이 이해할 수있는 높은 수준의 개념, DNN 출력에 이르기까지 인과 관계를 이해할 수 있습니다. 비판적으로, 그러한 모델이 정확하게 구성된다면, 반 추적 쿼리와 같은 다른 방법에 의해 지원되지 않는 DNN에 대한 내성적 쿼리를 지원할 수있다.
예를 들어, 보행자에게 보행자가 없었을 때 자동차가 우회전했을 가능성은 얼마입니까?
입력 이미지?”이러한 기능은 AI 시스템의 디버깅, 바이어스 이해 및 안전한 작동을위한 강력한 도구입니다.

이 논문에서 우리는이 가설을 탐구하고 DNN 설명에 대한 인과 적 접근이 가능하며 다양한 분류 시스템에 대한 귀중한 정보를 산출 함을 보여줍니다. 이 접근법의 일부로, DNN에서 저 차원 개념을 추출하여 사람이 이해할 수있는 "어휘"변수를 생성합니다. 그런 다음 중재 실험을 수행하여 DNN의 입력을 개념과 DNN의 출력에 연결하는 그래픽 원인 모델 (예 : Bayesian 네트워크 [Pearl, 2009])을 학습합니다. 마지막으로, 출력에 예상되는 인과 관계 영향이 가장 높은 여러 네트워크의 개념을 식별하여 이러한 모델의 설명력을 설명합니다. 우리의 기여는 다음과 같이 요약 될 수 있습니다 :

• 설명을 돕기 위해 인간이 이해할 수있는 개념으로 공식화 된 DNN 운영의 인과 모델
• 인간이 이해할 수있는 가능성이 높은 DNN에서 개념을 추출하는 감독되지 않은 기술
• DNN의 출력에 대한 입력 및 개념의 인과 적 영향을 측정하기 위해 제안 된 방법

본 논문의 나머지 부분은 다음과 같이 구성되어있다. 섹션 2에서는 관련 설명 작업에 대해 논의합니다. 3 장에서 우리는 DNN에서 인과 관계 개념을 공식화한다. 우리는 섹션 4에서 DNN에서 인간이 이해할 수있는 개념을 추출하는 것에 대해 논의합니다. 우리는 섹션 5에서 몇 가지 실험을 수행하고 결과의 예를 제시하고 섹션 6에서 결론을 내립니다.


2. 관련연구

최근의 많은 작품들은 시각적 설명을 위해 현역을 사용합니다.
이전의 몇몇 연구는 영향력있는 값을 가진 픽셀을 강조함으로써 예측을 시각화했습니다 (즉, 픽셀 값이 크게 변경되면 네트워크의 출력도 크게 변경됨). 이 영역에서의 초기 연구 [Erhan et al., 2009]에서, 관심있는 뉴런을 최대한 활성화시키는 입력 이미지는 이미지 도메인의 기울기 상승에 의해 발견되었습니다. 이 작업은 [Simonyan et al., 2013]에서 확장되어 클래스 별 돌출 성 맵을 얻었습니다. 이미지 공간을 조작하고 출력에 미치는 영향을 모니터링하는 것 외에도 네트워크의 작동 방식에 대한 이해를 돕기 위해 네트워크의 학습 된 기능에 대한 분석을 고려한 다른 연구가 있습니다. [Zeiler and Fergus, 2014]의 주요 작업에서 다중 레이어 디컨 볼 루션 네트워크를 사용하여 피쳐 활성화 (이미지의 컨벌루션 맵에서 출력)를 입력 픽셀 공간으로 다시 투영했습니다. 두 번째 컨볼 루션 레이어로 흐르는 그라데이션 정보는 GradCAM [Selvaraju et al., 2016]에서 입력 이미지의 식별 패턴을 식별하는 데 사용되었습니다. 네트워크 내에서 입력 이미지 및 그 활성화를 조작하는 대신에, 네트워크에서 원하는 응답을 생성하는 이미지를 생성하는 방법이 연구되었다; 구성된 결과는 어떤 입력이 네트워크에서 원하는 최대 응답을 갖는지 설명하는 데 도움이 될 수 있습니다. 네트워크를 동일하게 활성화하거나 [Mahendran and Vedaldi, 2015] 또는 최대로 [Erhan et al., 2009] 이미지를 구성하여 네트워크를 반전시키는 기술이 개발되었습니다. 네트워크에서 딥 컨볼 루션 필터를 최대한 활성화하는 매혹적인 이미지를 구축 할 수 있습니다 [Mordvintsev et al., 2015]. 더 큰 딥 넷 아키텍처에서 생성 된 정교한 기능을 보여줍니다. 그라디언트에 의존하지 않는 방법들도 최근 시각적 설명을위한 견인력을 얻었습니다. Layer-Wise Relevance Propagation [Bach et al., 2015]은 보존 공간에 의존하여 입력 공간의 각 요소에 대한 관련성 점수가 계산 될 때까지 네트워크 예측을 거꾸로 재분배하고 개인을 설명하기 위해 해석 가능한 히트 맵을 생성하는 것으로 나타났습니다 분류 [Binder et al., 2016]. [Xie et al., 2017]에보고 된 방법은 디컨 볼 루션 및 마스킹 기반 기술을 사용하여 후기 기능 맵에서 입력 개념의 분산 표현의 강도를 찾아서 스코어링함으로써 인간이 이해할 수있는 개념을 네트워크 출력과 연관시키는 것을 목표로합니다. 다른 방법들은 네트워크를 직접적으로 고려하지 않고, 더 간단하고 설명 가능한 모델을 위해“블랙 박스”모델에 대해 로컬에 근사한 것으로 사용자에 대한 신뢰를 불러 일으키는 결과를 보여 주었다 [Ribeiro et al., 2016].


3 인과모형

인과 관계는 AI에서 오랜 역사를 가지고 있으며 수많은 ML 노력은 통계적 모델 대신 현실적이고 정확한 인과 관계 모델을 구축하는 데 중점을두고있다 [Pearl, 2018]. 이로 인해 다양한 분야에서 많은 인과 적 형식주의와 의미론이 생겨났다 [Granger, 1980]. 그러나이 연구에서 우리는 Pearl의 미적분학의 개입 효과 측면에서 인과 관계의 의미론을 구성한다 [Pearl, 2009]. 변수 X = x1, ..., xn에 인과 관계 다이어그램을 정의하는 지시 된 그래픽 모델 G의 경우, X에 대한 관절 확률 분포를 (Pear Eqn 3.5에서)


여기서 pai는 xi의 부모입니다. do (x 0 i)로 표시된 xi = x 0 i를 설정하여 G에 대한 개입은 pai에서 xi까지의 가장자리가 제거되는 수정 된 그래프 G0을 유도하여 중재 후 분포 (Pearl Eqn. 3.10)를 생성합니다.


인과 관계의 의미는 본질적으로 설명이 인과 관계 모델이어야하기 때문에 DNN을 설명하는 데 중요합니다. 즉, 네트워크의 결정에 대한 설명을 찾을 때 "출력이 변경되거나 동일하게 유지되도록 입력에 어떤 변경을 가할 수 있습니까?"와 같은 질문을합니다. 이 원인의 공식화
문헌에 대한 설명이 잘 뒷받침되어있다 [Woodward, 2005]. DNN 출력 O, 입력 P 및 중간 변수 X에 대한 조인트 분포 P (O, P, X)를 정의하는 원인 모델을 고려합니다.


더 중요한 것은 개입 개념은 사용자가 DNN이 다른 출력 값을 생성하는 이유를 이해할 수 있도록 명확하고 수학적으로 확실한 메커니즘을 제공한다는 것입니다. 다시 말해, 관찰 된 DNN 출력에 대한 설명은 개입으로 공식화 할 수 있습니다. 예를 들어,“DNN은 머리를 보았 기 때문에 이미지에서 보행자를 인식했습니다”라고 말하면 DNN은 이미지에 보행자와 머리가 모두 있다고 생각합니다. DNN이 머리를 감지하지 못했거나 이미지에서 머리를 제거하기 위해 입력에 개입하면 보행자를 감지 할 확률이 변경되었을 것입니다.


DNN, 특히 이미지 분류 작업에 대한 기존 설명 방법은이 구체적인 인과 적 해석을 사용자에게 제공 할 수있는 능력이 부족합니다. 예를 들어 LRP (layerwise relevance propagation) [Bach et al., 2015] 및 Grad–CAM [Selvaraju et al., 2016]과 같은 그래디언트 기반 설명 방법은
특정 입력 Pj에 대해 실현 된 입력 P 활성화와 관련하여 클래스 C의 출력 활성화. 이러한 방법은 확실히 유용하지만 강력한 설명에 충분한 인과 중재 의미를 제공하지는 않습니다. 불연속 및 포화 그라디언트로 인해 네트워크에 의해 정의 된 함수가 선형 함수로 근사 될 수있는 제한된 도메인에 대한 인과성 만 나타냅니다. 기울기 하강을 사용하여 생성 된 적대적 사례 [Goodfellow et al., 2014]는 훈련 된 DNN에 의해 ​​정의 된 기능의 국부적 행동이 의미 상 관련성이 없음을 나타냅니다
이는 작은 영역으로 제한되는 기울기 기반 방법으로 정의 된 개입 외에도 DNN의 의미 상 모호한 측면을 고려한다는 점에서 해석 할 수 없음을 시사합니다. LRP와 같은 방법은 활성화 레벨을 가장 관련있는 픽셀에 "재분배"함으로써 그래디언트 기반 방법의 실제 문제 중 일부를 피하지만 효과적인 설명을 위해 원하는 명시 적 인과 중재 의미를 제공하지는 않습니다.
