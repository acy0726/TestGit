연구방법 


kinect - 마이크로소프트사에서 개발한 모션인식 플랫폼,  깊이 카메라 모듈 장착으로 깊이에 대한 정보를 추출 하고 
RGB영상에서 관절 추적정보를 제공한다.
YOLO 네트워크 - 딥러닝을 이용한 이미지 객체분류 알고리즘, 각 이미지를 SxS개의 그리드로 분할하고 그리드의
신뢰도를 계산하여 이미지안의 개체를 높은 확률로 구분한다.

0. 사과를 먹는다, 바나나를 먹는다, 물병으로 마신다, 컵으로 커피를 마신다 4가지의 행위로 테스트 진행

1. kinect v1, sdk 를 활용하여 RGB-D 영상과 골격 모델 관점 점 좌표 수집

1. RGB-D(깊이영상) 데이터스트림에서 YOLO 네트워크를 사용하여 물체 클래스 라벨, 물체 위치 정보 추출 하여 
사람과 물체간의 상호작용 패턴 을 구하기 위한 물체 데이터 수집

2. 골격 모델의 3D 좌표값을 2D  2D RGB 영상의 좌표값과 매칭 하기위해 3D 좌표값을 
kinect sdk을 이용하여 2D 좌표로 변환하여 RGB영상의 좌표값을 추출

3. 본논문에서 고려한 행동범위는 상체동작이므로 얼굴과 손의 위치의 골격모델의 관절점 위치 정보만 해당

4. YOLO 네트워크를 통해 추출된 개체 위치, 개체 라벨 정보와 RGB 영상의 좌표값을 이용하여 상호작용 행동의
목적을 추정하기위해 손괄점의 좌표값과 물체의 위치 좌표의 유클리디안 거리를 계산하여 
시계열 맥락 데이터를 구한다.


5. 시계열 맥락 데이터를 이용하여 행동의 목적 클래스를 분류하기위해 LSTM 순환신경망을 활용하여
행동목적을 분류한다.

6. 물체-사람 간의 위치에 대한 탐지된 경우 행동 목적의 분류가 가능하고


